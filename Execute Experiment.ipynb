{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a629df61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import copy\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, mean_squared_error\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy.stats import norm\n",
    "from bisect import bisect_left\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "from keras import Input, Model\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Conv2D, Conv1D, Flatten, MaxPooling2D, concatenate, Add\n",
    "from keras.layers import BatchNormalization, Dropout, SeparableConv2D, AvgPool2D\n",
    "from keras import Input\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e315b142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(X_train, X_test):\n",
    "    scaler = MinMaxScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1598bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_CNN_model(conv_layers, filters, size, pad, pool_func, pool_size, pool_stride):\n",
    "    global my_lr\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    for layer in range(conv_layers):\n",
    "        if pad==True:\n",
    "            model.add(Conv2D(filters, size, activation=None, padding='same'))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Activation('relu'))\n",
    "            if pool_func=='max':\n",
    "                model.add(MaxPooling2D((pool_size,pool_size), strides=pool_stride, padding='same'))\n",
    "            elif pool_func=='average':\n",
    "                model.add(AvgPool2D((pool_size,pool_size), strides=pool_stride, padding='same'))\n",
    "        elif pad==False:\n",
    "            model.add(Conv2D(filters, size, activation=None, padding='valid'))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Activation('relu'))\n",
    "            if pool_func=='max':\n",
    "                model.add(MaxPooling2D((pool_size,pool_size), strides=pool_stride, padding='valid'))\n",
    "            elif pool_func=='average':\n",
    "                model.add(AvgPool2D((pool_size,pool_size), strides=pool_stride, padding='valid'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.05))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    my_opt = keras.optimizers.SGD(learning_rate = my_lr)\n",
    "    \n",
    "    model.compile(optimizer=my_opt, loss='mse', metrics=['mse'])\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d1a0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(hidden_layers, neurons, nonlinearity):\n",
    "    global my_lr\n",
    "    #initiate model\n",
    "    model = Sequential()\n",
    "    #create hidden layers\n",
    "    for num_layers in range(hidden_layers):\n",
    "        model.add(Dense(neurons, nonlinearity))\n",
    "    #output layer\n",
    "    model.add(Dense(1, 'linear'))\n",
    "\n",
    "    #compile model\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=my_lr), loss=['mse'], metrics=['mse'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2349446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bootstrap(X_train, y_train, X_test, y_test, resamples):\n",
    "    global layers, nodes, activation, size, pooling, padding, filters, pool_size, pool_stride\n",
    "    global my_rng, my_epochs, my_batch, dataset, CC\n",
    "    \n",
    "    indices = np.zeros(((len(X_train),resamples)))\n",
    "    oob_indices = {k: [] for k in range(resamples)}\n",
    "  \n",
    "    for b in range(resamples):\n",
    "        indices[:, b] = resample(np.arange(len(X_train)), replace=True, n_samples=len(X_train), random_state = my_rng)\n",
    "        oob_indices[b] = np.setdiff1d(np.arange(len(X_train)), indices[:,b],assume_unique=True)\n",
    "\n",
    "    test_preds = np.zeros((len(X_test),resamples))\n",
    "    errors = np.zeros(resamples)\n",
    "    for b in range(resamples):\n",
    "        #finding right datasets\n",
    "        resampled_X = tf.convert_to_tensor(X_train[indices[:,b].astype(int)])\n",
    "        resampled_y = tf.convert_to_tensor(y_train[indices[:,b].astype(int)])\n",
    "        oob_X = tf.convert_to_tensor(X_train[oob_indices[b].astype(int)])\n",
    "        oob_y = y_train[oob_indices[b].astype(int)]\n",
    "        #define and fit model\n",
    "        if dataset == 'RotNIST':\n",
    "            model_b = define_CNN_model(conv_layers=layers, filters=filters, kernel_size=size, \n",
    "                                       pad = padding, pool_func=pooling, pool_size=pool_size, pool_stride=pool_stride) \n",
    "        else:\n",
    "            model_b = define_model(hidden_layers = layers, neurons = nodes, nonlinearity = activation)\n",
    "        model_b.fit(resampled_X, resampled_y, epochs=my_epochs, batch_size=my_batch, verbose=0)\n",
    "        #estimating data noise \n",
    "        oob_preds_b = copy.deepcopy(model_b.predict(oob_X, batch_size=len(oob_X)))\n",
    "        model_b_errors = np.array(oob_y).flatten() - np.array(oob_preds_b).flatten()\n",
    "        model_b_rmse = np.sqrt(np.sum(model_b_errors**2)/(len(model_b_errors)-1))\n",
    "        errors[b] = copy.deepcopy(model_b_rmse)\n",
    "        #test preds\n",
    "        test_preds_b = copy.deepcopy(model_b.predict(tf.convert_to_tensor(X_test), batch_size=len(X_test)))\n",
    "        test_preds[:,b] = copy.deepcopy(test_preds_b.flatten())\n",
    "        if (b+1) % 10 == 0: \n",
    "            print(\"{} models done!\".format(b+1))\n",
    "             \n",
    "    data_noise = np.mean(errors)     \n",
    "    \n",
    "    ensemble_preds = np.mean(test_preds, axis=1)\n",
    "    ensemble_errors = y_test.flatten() - ensemble_preds.flatten()\n",
    "    rmse_k = np.sqrt(np.sum(ensemble_errors**2)/np.size(ensemble_errors))\n",
    "    ensemble_std = np.std(test_preds, axis=1)\n",
    "\n",
    "    boot_bounds = np.zeros((len(X_test),5))\n",
    "    boot_bounds[:,0] = ensemble_preds + norm.ppf(0.5-CC/2) * np.sqrt(ensemble_std**2 + data_noise**2)\n",
    "    boot_bounds[:,1] = ensemble_preds + norm.ppf(0.5+CC/2) * np.sqrt(ensemble_std**2 + data_noise**2)\n",
    "    boot_bounds[:,2] = y_test.flatten()\n",
    "    boot_bounds[:,3] = (y_test.flatten() - boot_bounds[:,0])*(boot_bounds[:,1] - y_test.flatten()) > 0\n",
    "    boot_bounds[:,4] = boot_bounds[:,1] - boot_bounds[:,0]\n",
    "    \n",
    "    return boot_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7a6c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_perc_bootstrap(X_train, y_train, X_test, y_test, resamples):\n",
    "    global layers, activation, nodes, size, pooling, padding, filters, pool_size, pool_stride\n",
    "    global my_rng, my_epochs, my_batch, dataset, CC\n",
    "    \n",
    "    indices = np.zeros(((len(X_train),resamples)))\n",
    "    oob_indices = {k: [] for k in range(resamples)}\n",
    "  \n",
    "    for b in range(resamples):\n",
    "        indices[:, b] = resample(np.arange(len(X_train)), replace=True, n_samples=len(X_train), random_state = my_rng)\n",
    "        oob_indices[b] = np.setdiff1d(np.arange(len(X_train)), indices[:,b],assume_unique=True)\n",
    "\n",
    "    test_preds = np.zeros((len(X_test),resamples))\n",
    "    errors = np.zeros(resamples)\n",
    "    for b in range(resamples):\n",
    "        #finding right datasets\n",
    "        resampled_X = tf.convert_to_tensor(X_train[indices[:,b].astype(int)])\n",
    "        resampled_y = tf.convert_to_tensor(y_train[indices[:,b].astype(int)])\n",
    "        oob_X = tf.convert_to_tensor(X_train[oob_indices[b].astype(int)])\n",
    "        oob_y = y_train[oob_indices[b].astype(int)]\n",
    "        #define and fit model\n",
    "        if dataset == 'RotNIST':\n",
    "            model_b = define_CNN_model(conv_layers=layers, filters=filters, kernel_size=size, \n",
    "                                       pad = padding, pool_func=pooling, pool_size=pool_size, pool_stride=pool_stride) \n",
    "        else:\n",
    "            model_b = define_model(layers, nodes, activation)\n",
    "        model_b.fit(resampled_X, resampled_y, epochs=my_epochs, batch_size=my_batch, verbose=0)\n",
    "        #estimating data noise \n",
    "        oob_preds_b = copy.deepcopy(model_b.predict(oob_X, batch_size=len(oob_X)))\n",
    "        model_b_errors = np.array(oob_y).flatten() - np.array(oob_preds_b).flatten()\n",
    "        random_errors = resample(model_b_errors, replace=True, n_samples=len(X_test), random_state=my_rng).flatten()\n",
    "        #test preds\n",
    "        test_preds_b = model_b.predict(X_test, batch_size=len(X_test)).flatten()\n",
    "        test_preds[:,b] = test_preds_b + random_errors\n",
    "        if (b+1) % 10 == 0: \n",
    "            print(\"{} models done!\".format(b+1))\n",
    "\n",
    "    boot_bounds = np.zeros((len(X_test),5))\n",
    "    boot_bounds[:,0] = np.percentile(test_preds, (0.5 - CC/2), axis=1)\n",
    "    boot_bounds[:,1] = np.percentile(test_preds, (0.5 + CC/2), axis=1)\n",
    "    boot_bounds[:,2] = y_test.flatten()\n",
    "    boot_bounds[:,3] = (y_test.flatten() - boot_bounds[:,0])*(boot_bounds[:,1] - y_test.flatten()) > 0\n",
    "    boot_bounds[:,4] = boot_bounds[:,1] - boot_bounds[:,0]\n",
    "    \n",
    "    return boot_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cafd705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_conformal(X_train, y_train, X_test, y_test):\n",
    "    global grid_fineness, layers, nodes, activation, size, pooling, padding, filters, pool_size, pool_stride\n",
    "    global my_epochs, my_batch, trial_bandwidths, search_range, dataset, CC\n",
    "\n",
    "    n = len(X_train)\n",
    "    results = np.zeros((len(X_test),4,len(trial_bandwidths)))\n",
    "    \n",
    "    #train a network on the original training set... use predictions of test set to center PIs\n",
    "    if dataset == 'RotNIST':\n",
    "        model = define_CNN_model(conv_layers=layers, filters=filters, kernel_size=size, \n",
    "                                 pad = padding, pool_func=pooling, pool_size=pool_size, pool_stride=pool_stride) \n",
    "    else:\n",
    "        model = define_model(layers, nodes, activation)\n",
    "    model.fit(x = tf.convert_to_tensor(X_train),\n",
    "              y = tf.convert_to_tensor(y_train),\n",
    "              epochs=my_epochs,\n",
    "              batch_size=my_batch,\n",
    "              verbose=0)\n",
    "    original_y_preds = model.predict(tf.convert_to_tensor(X_test)).flatten()\n",
    "    \n",
    "    pi = np.zeros((len(X_test),grid_fineness, len(trial_bandwidths)))\n",
    "    candidates = np.zeros((len(X_test),grid_fineness))\n",
    "    for obs in X_test.index:        \n",
    "        #establish the search grid of candidate target values using the predicted y value and search range\n",
    "        search_grid = np.linspace(start = original_y_preds[obs]-0.5*search_range,\n",
    "                                  stop = original_y_preds[obs]+0.5*search_range,\n",
    "                                  num=grid_fineness)\n",
    "        candidates[obs,:] = search_grid\n",
    "        #for each obs in test set, append it to the training feature set\n",
    "        x = X_test.iloc[obs,:].values\n",
    "        temp_X_train = X_train.copy(deep=True)\n",
    "        temp_X_train.loc[len(temp_X_train.index)] = x\n",
    "\n",
    "        #for each trial value y in the search grid, append it to the training target values\n",
    "        #then fit the model \n",
    "        #then take residuals\n",
    "        for trial, y in enumerate(search_grid):\n",
    "            temp_y_train = np.concatenate((y_train, np.array([y])))\n",
    "            if dataset == 'RotNIST':\n",
    "                model = define_CNN_model(conv_layers=layers, filters=filters, kernel_size=size, \n",
    "                                         pad = padding, pool_func=pooling, pool_size=pool_size, pool_stride=pool_stride) \n",
    "            else:\n",
    "                model = define_model(layers, nodes, activation)\n",
    "            model.fit(x = tf.convert_to_tensor(temp_X_train),\n",
    "                      y = tf.convert_to_tensor(temp_y_train),\n",
    "                      epochs=my_epochs,\n",
    "                      batch_size=my_batch,\n",
    "                      verbose=0)\n",
    "            #predict target values for original training set and calc resids\n",
    "            preds_y_train = model.predict(tf.convert_to_tensor(temp_X_train), batch_size=len(temp_X_train))\n",
    "            preds_y_train = preds_y_train\n",
    "            resid = temp_y_train.flatten() - preds_y_train.flatten()\n",
    "            \n",
    "            for h, my_bandwidth in enumerate(trial_bandwidths):\n",
    "                if my_bandwidth == 0:\n",
    "                    calibration_scores = np.abs(resids[:-1])\n",
    "                    critical_score = np.abs(resids[-1])\n",
    "                else:\n",
    "                    kde = KernelDensity(bandwidth=my_bandwidth).fit(resids.reshape(-1,1))\n",
    "                    calibration_scores = -1 * kde.score_samples(resids.reshape(-1,1))\n",
    "                    critical_score = -1 * kde.score_samples([[resids[-1]]])\n",
    "                #now compare the residuals from the original training set to that of the trial y value\n",
    "                pi[obs,trial,h] = (1 + np.sum(calibration_scores <= critical_score)) / (1 + len(calibration_scores))\n",
    "                \n",
    "    full_conformal = np.zeros((len(y_test),4,len(trial_bandwidths))) \n",
    "    full_conformal[:,1,:] = y_test.reshape(-1,1)\n",
    "    full_conformal[:,0,:] = candidates.mean(axis=1).reshape(-1,1)\n",
    "\n",
    "    for obs in range(len(y_test)):\n",
    "        y_true = y_test[obs]\n",
    "\n",
    "        for h in range(len(trial_bandwidths)):\n",
    "            try:\n",
    "                pred_region = pi[obs,:,h] < 0.95\n",
    "                full_conformal[seed,obs,1,h] = search_range / grid_fineness * pred_region.sum()\n",
    "                upper_arg = np.argwhere(candidates[obs,:] <= y_true).flatten().max()\n",
    "                lower_arg = np.argwhere(candidates[obs,:] > y_true).flatten().min()\n",
    "                full_conformal[seed,obs,2,h] = pred_region[lower_arg] * pred_region[upper_arg]\n",
    "            except:\n",
    "                full_conformal[seed,obs,2,h] = 0 \n",
    "    \n",
    "    return full_conformal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927a27a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_conformal(X_train, y_train, X_test, y_test):\n",
    "    global activation, layers, nodes, size, pooling, padding, filters, pool_size, pool_stride\n",
    "    global my_rng, my_epochs, my_batch, my_lr, grid_fineness, search_range, dataset, trial_bandwidths, CC\n",
    "        \n",
    "    #create proper training and calibration sets\n",
    "    X_proper_train, X_calibration, y_proper_train, y_calibration = train_test_split(X_train, y_train,\n",
    "                                                                                    test_size=0.5,\n",
    "                                                                                    random_state=my_rng)\n",
    "    \n",
    "    #fit model to proper training data\n",
    "    if dataset == 'RotNIST':\n",
    "        model = define_CNN_model(conv_layers=layers, filters=filters, kernel_size=size, \n",
    "                                 pad = padding, pool_func=pooling, pool_size=pool_size, pool_stride=pool_stride) \n",
    "    else:\n",
    "        model = define_model(layers, nodes, activation)\n",
    "    model.fit(tf.convert_to_tensor(X_proper_train), \n",
    "              tf.convert_to_tensor(y_proper_train),\n",
    "              epochs=my_epochs, \n",
    "              batch_size=my_batch, \n",
    "              verbose=0)\n",
    "\n",
    "    #predict values in test set \n",
    "    test_preds = model.predict(tf.convert_to_tensor(X_test), batch_size=len(X_test)).flatten()\n",
    "\n",
    "    #identify calibration sets and estimate residuals\n",
    "    y_cal_preds = model.predict(tf.convert_to_tensor(X_calibration), batch_size=len(X_calibration))\n",
    "    residuals = y_calibration.values.flatten() - y_cal_preds.flatten()\n",
    "    \n",
    "    #fit KDE distributions on augmented residual sets and compute p-value of potential residual values\n",
    "    search_grid = np.linspace(0-search_range/2,\n",
    "                              0+search_range/2,\n",
    "                              grid_fineness)\n",
    "    p_values = np.zeros((len(search_grid),len(trial_bandwidths)))\n",
    "    for trial, resid_trial in enumerate(search_grid):\n",
    "        aug_resids = np.concatenate([residuals, np.array([resid_trial])])\n",
    "        for bandwidth, my_bandwidth in enumerate(trial_bandwidths):\n",
    "            if my_bandwidth == 0:\n",
    "                conform_scores = np.abs(aug_resids)\n",
    "            else:\n",
    "                kde = KernelDensity(bandwidth=my_bandwidth).fit(aug_resids.reshape(-1,1))\n",
    "                conform_scores = -kde.score_samples(aug_resids.reshape(-1,1))\n",
    "            p_values[trial,bandwidth] = np.sum(conform_scores <= conform_scores[-1]) / np.size(conform_scores)\n",
    "    \n",
    "    #initialize results array\n",
    "    results = np.zeros((len(y_test),4,len(trial_bandwidths)))\n",
    "    results[:,1,:] = y_test.reshape(-1,1)\n",
    "    results[:,0,:] = test_preds.reshape(-1,1)\n",
    "    \n",
    "    #loop over each test value and bandwidth to find and evaluate prediction regions\n",
    "    for h, my_bandwidth in enumerate(trial_bandwidths):\n",
    "        mean_p_values = p_values[:,h]\n",
    "        pred_region = mean_p_values < CC\n",
    "        #width\n",
    "        results[:,3,h] = np.sum(pred_region) / grid_fineness * search_range\n",
    "        for test_obs, y_true in enumerate(y_test):\n",
    "            critical_resid = y_true - test_preds[test_obs]\n",
    "            try:\n",
    "                lower_arg = np.argwhere(search_grid < critical_resid).flatten().max()\n",
    "                upper_arg = np.argwhere(search_grid >= critical_resid).flatten().min()\n",
    "                #in bounds?\n",
    "                results[test_obs,2,h] = pred_region[lower_arg] * pred_region[upper_arg]\n",
    "            except:\n",
    "                #in bounds? \n",
    "                results[test_obs,2,h] = 0\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d272469b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_conformal(X_train, y_train, X_test, y_test, folds):\n",
    "    global my_rng, activation, layers, nodes, size, pooling, padding, filters, pool_size, pool_stride\n",
    "    global my_epochs, my_batch, my_lr, grid_fineness, search_range, dataset, trial_bandwidths, CC\n",
    "     \n",
    "    #initiate models\n",
    "    model_dict = {}\n",
    "    for k in range(folds):\n",
    "        if dataset == 'RotNIST':\n",
    "            exec(\"model_{} = define_CNN_model({},{},{},{},{},{},{})\".format(k,layers,filters,size,padding,pooling,pool_size,pool_stride))\n",
    "        else:\n",
    "            exec(\"model_{} = define_model({},{},'{}')\".format(k,layers,nodes,activation))\n",
    "        exec(\"model_dict[{}] = model_{}\".format(k,k))    \n",
    "    \n",
    "    #create folds and fit models to each set of training data; \n",
    "    #then calculate residuals on calibration sets\n",
    "    residual_sets = {}\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=my_rng)\n",
    "    counter = 0\n",
    "    for train_index, cal_index in kf.split(X_train):\n",
    "        X_train_k, X_cal_k = X_train[train_index], X_train[cal_index]\n",
    "        y_train_k, y_cal_k = y_train[train_index], y_train[cal_index]\n",
    "        model_dict[counter].fit(tf.convert_to_tensor(X_train_k), tf.convert_to_tensor(y_train_k), \n",
    "                                epochs=my_epochs, batch_size=my_batch, verbose=0)\n",
    "        y_pred_b = model_dict[counter].predict(tf.convert_to_tensor(X_cal_k), batch_size=len(X_cal_k)).flatten()\n",
    "        residual_sets[counter] = y_cal_k.flatten() - y_pred_b.flatten()       \n",
    "        counter+=1\n",
    "\n",
    "    #predict values in test set using ensemble of fitted networks\n",
    "    test_preds = np.zeros((len(y_test),folds))\n",
    "    for b in range(folds):\n",
    "        test_preds[:,b] = model_dict[b].predict(tf.convert_to_tensor(X_test), batch_size=len(X_test)).flatten()\n",
    "    ensemble_mean = test_preds.mean(axis=1)\n",
    "  \n",
    "    #fit KDE distributions on augmented residual sets and compute p-value of potential residual values\n",
    "    search_grid = np.linspace(0-search_range/2,\n",
    "                              0+search_range/2,\n",
    "                              grid_fineness)\n",
    "    p_values = np.zeros((len(search_grid),folds,len(trial_bandwidths)))\n",
    "    for b in range(folds):\n",
    "        residuals = residual_sets[b]\n",
    "        for trial, resid_trial in enumerate(search_grid):\n",
    "            aug_resids = np.concatenate([residuals, np.array([resid_trial])])\n",
    "            for bandwidth, my_bandwidth in enumerate(trial_bandwidths):\n",
    "                if my_bandwidth == 0:\n",
    "                    conform_scores = np.abs(aug_resids)\n",
    "                else:\n",
    "                    kde = KernelDensity(bandwidth=my_bandwidth).fit(aug_resids.reshape(-1,1))\n",
    "                    conform_scores = -kde.score_samples(aug_resids.reshape(-1,1))\n",
    "                p_values[trial,b,bandwidth] = np.sum(conform_scores <= conform_scores[-1]) / np.size(conform_scores)\n",
    "\n",
    "    #initialize results array\n",
    "    results = np.zeros((len(y_test),4,len(trial_bandwidths)))\n",
    "    results[:,1,:] = y_test.reshape(-1,1)\n",
    "    results[:,0,:] = test_preds.reshape(-1,1)\n",
    "    \n",
    "    #loop over each test value and bandwidth to find and evaluate prediction regions\n",
    "    for h, my_bandwidth in enumerate(trial_bandwidths):\n",
    "        mean_p_values = p_values[:,:,h].mean(axis=1)\n",
    "        pred_region = mean_p_values < CC\n",
    "        #width\n",
    "        results[:,3,h] = np.sum(pred_region) / grid_fineness * search_range\n",
    "        for test_obs, y_true in enumerate(y_test):\n",
    "            critical_resid = y_true - test_preds[test_obs]\n",
    "            try:\n",
    "                lower_arg = np.argwhere(search_grid < critical_resid).flatten().max()\n",
    "                upper_arg = np.argwhere(search_grid >= critical_resid).flatten().min()\n",
    "                #in bounds?\n",
    "                results[test_obs,2,h] = pred_region[lower_arg] * pred_region[upper_arg]\n",
    "            except:\n",
    "                #in bounds? \n",
    "                results[test_obs,2,h] = 0\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9da81bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_conformal(X_train, y_train, X_test, y_test, B):\n",
    "    global my_rng, activation, layers, nodes, size, pooling, padding, filters, pool_size, pool_stride\n",
    "    global my_epochs, my_batch, my_lr, grid_fineness, search_range, dataset, trial_bandwidths\n",
    "\n",
    "    #create lists of indices for resampled datasets and their out-of-bag sets\n",
    "    indices = np.zeros(((len(X_train),resamples)))\n",
    "    oob_indices = {k: [] for k in range(resamples)}\n",
    "    for b in range(resamples):\n",
    "        indices[:, b] = resample(np.arange(len(X_train)), replace=True, n_samples=len(X_train), random_state = my_rng).astype(int)\n",
    "        oob_indices[b] = np.setdiff1d(np.arange(len(X_train)), indices[:,b],assume_unique=True).astype(int)\n",
    "        \n",
    "    #initiate models\n",
    "    model_dict = {}\n",
    "    for b in range(B):\n",
    "        if dataset == 'RotNIST':\n",
    "            exec(\"model_{} = define_CNN_model({},{},{},{},{},{},{})\".format(k,layers,filters,size,padding,pooling,pool_size,pool_stride))\n",
    "        else:\n",
    "            exec(\"model_{} = define_model({},{},'{}')\".format(k,layers,nodes,activation))\n",
    "        exec(\"model_dict[{}] = model_{}\".format(k,k))    \n",
    "    \n",
    "    #fit models to resampled datasets\n",
    "    for b in range(resamples):\n",
    "        resampled_X = tf.convert_to_tensor(X_train[indices[:,b].astype(int)])\n",
    "        resampled_y = tf.convert_to_tensor(y_train[indices[:,b].astype(int)])\n",
    "        model_b.fit(resampled_X, resampled_y, epochs=my_epochs, batch_size=my_batch, verbose=0)\n",
    "    \n",
    "    #identify calibration sets and estimate residuals\n",
    "    residual_sets = {k: [] for k in range(B)}\n",
    "    for b in range(B):\n",
    "        calibration_X_b = tf.convert_to_tensor(X_train[oob_indices[b].astype(int)])\n",
    "        calibration_y_b = tf.convert_to_tensor(y_train[oob_indices[b].astype(int)])\n",
    "        y_pred_b = model_dict[b].predict(calibration_X_b, batch_size=len(calibration_X_b)).flatten()\n",
    "        residual_sets[b] = calibration_y_b.flatten() - y_pred_b.flatten()\n",
    "\n",
    "    #predict values in test set using ensemble of fitted networks\n",
    "    test_preds = np.zeros((len(y_test),B))\n",
    "    for b in range(B):\n",
    "        test_preds[:,b] = model_dict[b].predict(tf.convert_to_tensor(X_test), batch_size=len(X_test)).flatten()\n",
    "    ensemble_mean = test_preds.mean(axis=1)\n",
    "    \n",
    "    #fit KDE distributions on augmented residual sets and compute p-value of potential residual values\n",
    "    search_grid = np.linspace(0-search_range/2,\n",
    "                              0+search_range/2,\n",
    "                              grid_fineness)\n",
    "    p_values = np.zeros((len(search_grid),folds,len(trial_bandwidths)))\n",
    "    for b in range(B):\n",
    "        residuals = residual_sets[b]\n",
    "        for trial, resid_trial in enumerate(search_grid):\n",
    "            aug_resids = np.concatenate([residuals, np.array([resid_trial])])\n",
    "            for bandwidth, my_bandwidth in enumerate(trial_bandwidths):\n",
    "                if my_bandwidth == 0:\n",
    "                    conform_scores = np.abs(aug_resids)\n",
    "                else:\n",
    "                    kde = KernelDensity(bandwidth=my_bandwidth).fit(aug_resids.reshape(-1,1))\n",
    "                    conform_scores = -kde.score_samples(aug_resids.reshape(-1,1))\n",
    "                p_values[trial,b,bandwidth] = np.sum(conform_scores <= conform_scores[-1]) / np.size(conform_scores)\n",
    "\n",
    "    #initialize results array\n",
    "    results = np.zeros((len(y_test),4,len(trial_bandwidths)))\n",
    "    results[:,1,:] = y_test.reshape(-1,1)\n",
    "    results[:,0,:] = test_preds.reshape(-1,1)\n",
    "    \n",
    "    #loop over each test value and bandwidth to find and evaluate prediction regions\n",
    "    for h, my_bandwidth in enumerate(trial_bandwidths):\n",
    "        mean_p_values = p_values[:,:,h].mean(axis=1)\n",
    "        pred_region = mean_p_values < CC\n",
    "        #width\n",
    "        results[:,3,h] = np.sum(pred_region) / grid_fineness * search_range\n",
    "        for test_obs, y_true in enumerate(y_test):\n",
    "            critical_resid = y_true - test_preds[test_obs]\n",
    "            try:\n",
    "                lower_arg = np.argwhere(search_grid < critical_resid).flatten().max()\n",
    "                upper_arg = np.argwhere(search_grid >= critical_resid).flatten().min()\n",
    "                #in bounds?\n",
    "                results[test_obs,2,h] = pred_region[lower_arg] * pred_region[upper_arg]\n",
    "            except:\n",
    "                #in bounds? \n",
    "                results[test_obs,2,h] = 0\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3ba361",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process digits data\n",
    "#follow instructions at the link below for obtaining the needed files\n",
    "#https://www.mathworks.com/help/deeplearning/ug/train-a-convolutional-neural-network-for-regression.html\n",
    "\n",
    "X_train = np.loadtxt('trainX.txt')\n",
    "X_train = np.reshape(X_train, (28, 5000, 28, 1))\n",
    "X_train = np.transpose(X_train, (1,0,2, 3))\n",
    "X_val = np.loadtxt('valX.txt')\n",
    "X_val = np.reshape(X_val, (28, 5000, 28, 1))\n",
    "X_val = np.transpose(X_val, (1,0,2, 3))\n",
    "digits = np.concatenate((X_train, X_val), axis=0)\n",
    "\n",
    "y_train = np.loadtxt('trainY.txt')\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_val = np.loadtxt('valY.txt')\n",
    "y_val = y_val.reshape(-1,1)\n",
    "labels = np.concatenate((y_train, y_val), axis=0)\n",
    "\n",
    "new_dim = 14\n",
    "\n",
    "new_digits = np.zeros((10000,new_dim,new_dim,1))\n",
    "\n",
    "for idx in range(len(digits)):\n",
    "    img = digits[idx,:,:,:]\n",
    "    new_img = cv2.resize(img, dsize=(new_dim,new_dim), interpolation=cv2.INTER_LINEAR)\n",
    "    new_digits[idx,:,:,0] = new_img\n",
    "    \n",
    "np.save('digit_images_14', new_digits)\n",
    "np.save('digit_images_28', digits)\n",
    "np.save('digit_labels', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be9865d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###          Key : [Format, Header_Row, Text_Delimiter, Target_Col, \n",
    "###                 Epochs, BatchSize, LearningRate, \n",
    "###                 Optimal_Layers, Optimal_Nodes, Optimal_Activation, \n",
    "###                 (CNNs: Optimal_layers, Optimal_KernelSize, Optimal_Filters, Optimal_Pooling)\n",
    "###                 Search Range,\n",
    "###                 Location (CNNs: input file location, target file location)]\n",
    "### NOTE: some dataset locations reference a local path; download these datasets first from the links provided in Table 1 (pg 58)\n",
    "data_dict = {'BostonHousing': [None, None, None, None, 200, 32, 0.001, 3, 100, 'relu', 26.27, None],\n",
    "             'WineQuality': ['csv',0,';', -1, 75, 32, 0.001, 2, 50, 'relu', 5.12,\n",
    "                             'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'],             \n",
    "             'ConcreteStrength': ['excel', 0, None, -1, 200, 32, 0.001, 3, 100, 'relu', 49.67,\n",
    "                             'https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete_Data.xls'],\n",
    "             'EnergyEfficiency': ['excel', 0, None, -2, 250, 32, 0.001, 3, 100, 'tanh', 8.49,\n",
    "                                  'https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx'],\n",
    "             'Kinematics': ['csv', 0, ',', -1, 150, 32, 0.001, 3, 75, 'tanh', 0.61,\n",
    "                            'dataset_2175_kin8nm.csv'],\n",
    "             'NavalPropulsion': ['csv', 0, ',', 1, 80, 256, 0.001, 3, 100, 'relu', 0.25,\n",
    "                                 'NavalPropulsion.csv'],\n",
    "             'PowerPlant': ['csv', 0, ',', -1, 350, 600, 0.01, 3, 50, 'relu', 32.01,\n",
    "                            'PowerPlant.csv'],\n",
    "             'ProteinStructure': ['csv', 0, ',', 0, 75, 1024, 0.025, 3, 50, 'tanh', 36.44,\n",
    "                                  'https://archive.ics.uci.edu/ml/machine-learning-databases/00265/CASP.csv'],\n",
    "             'YachtHydrodynamics': ['csv', 0, ',', -1, 500, 64, 0.001, 3, 100, 'relu', 9.12,\n",
    "                                    'yacht_hydrodynamics.csv'],\n",
    "             'YearPrediction': ['txt', None, ',', 0, 15, 4096, 0.1, 1, 50, 'relu', 119.83,\n",
    "                                'YearPredictionMSD.txt'],\n",
    "             'RotNIST': [None, None, None, None, 20, 64, 1e-3, 3, 5, 32, 'max', 52.34,\n",
    "                         'digit_images_14.npy', 'digit_labels.npy']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0c31ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_data(data_dict, dataset):\n",
    "    \n",
    "    file_format = data_dict[dataset][0]\n",
    "    header_row = data_dict[dataset][1]\n",
    "    text_delim = data_dict[dataset][2]\n",
    "    target_col = data_dict[dataset][3]\n",
    "    my_epochs = data_dict[dataset][4]\n",
    "    my_batch = data_dict[dataset][5]\n",
    "    my_lr = data_dict[dataset][6]\n",
    "    layers = data_dict[dataset][7]\n",
    "    if dataset == 'RotNIST':\n",
    "        kernel_size = data_dict[dataset][8]\n",
    "        filters = data_dict[dataset][9]\n",
    "        pooling = data_dict[dataset][10]\n",
    "        search_range = data_dict[dataset][11]\n",
    "        img_location = data_dict[dataset][12]\n",
    "        target_location = data_dict[dataset][13]\n",
    "        case_string = '{}_{}_{}_{}'.format(layers, kernel_size, pooling, filters)\n",
    "    else:\n",
    "        nodes = data_dict[dataset][8]\n",
    "        activation = data_dict[dataset][9]\n",
    "        search_range = data_dict[dataset][10]\n",
    "        location = data_dict[dataset][11]\n",
    "        case_string = '{}_{}_{}'.format(layers, nodes, activation)\n",
    "    \n",
    "    if dataset == 'BostonHousing':\n",
    "        (X,y), (temp,temp2) = keras.datasets.boston_housing.load_data(path=\"boston_housing.npz\", test_split=0, seed=113)\n",
    "    elif dataset== 'RotNIST':\n",
    "        X = np.load(img_location)\n",
    "        y = np.load(target_location)\n",
    "    else:\n",
    "        if file_format == 'excel':\n",
    "            data = pd.read_excel(location, header=header_row)\n",
    "        else:\n",
    "            data = pd.read_csv(location, delimiter=text_delim, header=header_row)\n",
    "        if dataset == 'EnergyEfficiency':\n",
    "            X = data.iloc[:,:-2].values\n",
    "            y = data.iloc[:,-2].values\n",
    "        else:\n",
    "            y = data.iloc[:,target_col]\n",
    "            X = data.drop(y.name, axis=1).values\n",
    "            y = y.values\n",
    "            \n",
    "    return X, y, case_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388ca204",
   "metadata": {},
   "outputs": [],
   "source": [
    "###STEP 1 EXPERIMENT\n",
    "\n",
    "CC = 0.95\n",
    "trial_bandwidths = [0]\n",
    "my_rng = np.random.RandomState(seed=2)\n",
    "test_folds = 5\n",
    "\n",
    "for dataset in data_dict.keys():\n",
    "    \n",
    "    results_dict = {}\n",
    "    \n",
    "    X, y, _ = find_data(data_dict, dataset)\n",
    "    \n",
    "    if dataset == 'RotNIST':\n",
    "        filters = 32\n",
    "        padding = True\n",
    "        pool_size = 2\n",
    "        pool_stride = 2\n",
    "        for layers in [2,3,4]:\n",
    "            for size in [1,3,5]:\n",
    "                for pooling in ['max','average']:\n",
    "                    case_string = '{}_{}_{}'.format(layers, size, pooling)\n",
    "                    \n",
    "                    kf = KFold(n_splits=test_folds, shuffle=True, random_state=my_rng)\n",
    "                    \n",
    "                    fold = 0\n",
    "                    for train_index, test_index in kf.split(X):\n",
    "                        X_train_k, X_test_k = X[train_index], X[test_index]\n",
    "                        y_train_k, y_test_k = y[train_index], y[test_index]\n",
    "                        results_dict['bootstrap_1000'] = run_bootstrap(X_train_k, y_train_k, X_test_k, y_test_k, 1000)\n",
    "                        results_dict['split_conformal'] = split_conformal(X_train_k, y_train_k, X_test_k, y_test_k)\n",
    "\n",
    "                        for method in results_dict.keys():\n",
    "                            df = results_dict[method]\n",
    "                            df.to_csv('Step1_{}_{}_fold{}_{}.csv'.format(dataset,method,fold,case_string))\n",
    "                        fold += 1\n",
    "    \n",
    "    else:\n",
    "        for layers in [1,2,3]:\n",
    "            for nodes in [5,10,25,50,75,100]:\n",
    "                for pooling in ['relu','sigmoid','tanh']:\n",
    "                    case_string = '{}_{}_{}'.format(layers, nodes, activation)\n",
    "                    \n",
    "                    kf = KFold(n_splits=folds, shuffle=True, random_state=my_rng)\n",
    "                    \n",
    "                    fold = 0\n",
    "                    for train_index, test_index in kf.split(X):\n",
    "                        X_train_k, X_test_k = X[train_index], X[test_index]\n",
    "                        y_train_k, y_test_k = y[train_index], y[test_index]\n",
    "                        results_dict['bootstrap_1000'] = run_bootstrap(X_train_k, y_train_k, X_test_k, y_test_k, 1000)\n",
    "                        results_dict['split_conformal'] = split_conformal(X_train_k, y_train_k, X_test_k, y_test_k)\n",
    "\n",
    "                        for method in results_dict.keys():\n",
    "                            my_array = results_dict[method]\n",
    "                            np.save('Step1_{}_{}_fold{}_{}.npy'.format(dataset,method,fold,case_string), my_array)\n",
    "                        fold += 1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb655e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### STEP 2 EXPERIMENT\n",
    "\n",
    "CC = 0.95\n",
    "test_size = 100\n",
    "trial_bandwidths = np.linspace(0,2,21)\n",
    "my_rng = np.random.RandomState(seed=2)\n",
    "\n",
    "for dataset in data_dict.keys():\n",
    "    \n",
    "    X, y, case_string = find_data(data_dict, dataset)\n",
    "    \n",
    "    results_dict = {}\n",
    "    for trial_seed in range(10):\n",
    "        test_idx = resample(np.arange(len(X)), replace=False, n_samples = test_size, random_state = trial_seed)\n",
    "        train_idx = np.setdiff1d(np.arange(len(X)), test_idx, assume_unique=True)\n",
    "        X_train, y_train = X[train_idx], y[train_idx]\n",
    "        X_test, y_test = X[test_idx], y[test_idx]\n",
    "        if dataset != 'RotNIST':\n",
    "            X_train, X_test = scale_data(X_train, X_test)\n",
    "        \n",
    "        grid_fineness = 1000\n",
    "        results_dict['bootstrap_100'] = run_bootstrap(X_train, y_train, X_test, y_test, 100)\n",
    "        results_dict['bootstrap_500'] = run_bootstrap(X_train, y_train, X_test, y_test, 500)\n",
    "        results_dict['perc_bootstrap_1000'] = run_perc_bootstrap(X_train, y_train, X_test, y_test, 1000)\n",
    "        results_dict['cross_conformal_5'] = cross_conformal(X_train, y_train, X_test, y_test, 5)\n",
    "        results_dict['cross_conformal_10'] = cross_conformal(X_train, y_train, X_test, y_test, 10)\n",
    "        results_dict['cross_conformal_20'] = cross_conformal(X_train, y_train, X_test, y_test, 20)\n",
    "        results_dict['bootstrap_conformal_5'] = bootstrap_conformal(X_train, y_train, X_test, y_test, 5)\n",
    "        results_dict['bootstrap_conformal_10'] = bootstrap_conformal(X_train, y_train, X_test, y_test, 10)\n",
    "        results_dict['bootstrap_conformal_20'] = bootstrap_conformal(X_train, y_train, X_test, y_test, 20)\n",
    "        results_dict['split_conformal'] = split_conformal(X_train, y_train, X_test, y_test)\n",
    "        \n",
    "        grid_fineness = 100\n",
    "        results_dict['full_conformal'] = run_full_conformal(X_train, y_train, X_test, y_test)\n",
    "        \n",
    "        for method in results_dict.keys():\n",
    "            my_array = results_dict[method]\n",
    "            np.save('Step2_{}_{}_seed{}_{}.npy'.format(dataset,method,trial_seed,case_string), my_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
